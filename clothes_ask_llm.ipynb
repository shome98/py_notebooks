{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39d70a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a0c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gemma_locally(save_path=\"D:/models/gemma-2b\", model_name=\"google/gemma-2b\"):\n",
    "    print(\"ðŸ“¥ Downloading and saving model locally...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    model.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"âœ… Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e1970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma_pipeline_pre(token, model_name=\"google/gemma-2b\"):\n",
    "    # Load tokenizer with token authentication\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)\n",
    "\n",
    "    # Load model with token authentication\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=token,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Create text generation pipeline\n",
    "    gemma_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gemma_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7b81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma_pipeline(token, model_path=\"D:/models/gemma-2b\", model_name=\"google/gemma-2b\"):\n",
    "    \"\"\"\n",
    "    Loads Gemma model from local path or Hugging Face with disk offloading.\n",
    "    \"\"\"\n",
    "\n",
    "    # If model not saved locally, download and save it first\n",
    "    if not os.path.exists(model_path):\n",
    "        save_gemma_locally(save_path=model_path, model_name=model_name)\n",
    "\n",
    "    print(\"ðŸ§  Loading Gemma model with disk offloading...\")\n",
    "\n",
    "    # Load tokenizer from local folder\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=token, trust_remote_code=True)\n",
    "\n",
    "    # Load model using disk offloading\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        offload_folder=\"./offload\",  # Folder for offloaded weights\n",
    "        offload_state_dict=True      # Enable full offloading\n",
    "    )\n",
    "\n",
    "    # Alternatively: use `accelerate` for more control\n",
    "    # model = load_checkpoint_and_dispatch(\n",
    "    #     model,\n",
    "    #     model_path,\n",
    "    #     device_map=\"auto\",\n",
    "    #     offload_folder=\"./offload\",\n",
    "    #     dtype=torch.float16\n",
    "    # )\n",
    "\n",
    "    # Create text generation pipeline\n",
    "    gemma_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gemma_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204ff47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.utilities import SQLDatabase\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "db_uri=os.getenv(\"POSTGRES_URI\")\n",
    "\n",
    "if db_uri is None:\n",
    "    raise ValueError(\"POSTGRES_URI ot found in .env\")\n",
    "\n",
    "db=SQLDatabase.from_uri(db_uri,sample_rows_in_table_info=3)\n",
    "\n",
    "print(db.table_info)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbb690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token=os.getenv(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"could not find hugging face token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cff6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.sql import SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db200017",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "hf_token=os.getenv(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"could not find hugging face token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = load_gemma_pipeline(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f40624",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "qns1 = db_chain(\"How many t-shirts do we have left for nike in extra small size and white color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1716f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tinyLlama_pipeline(token, model_path=\"D:/models/TinyLlama-1.1B-Chat-v1.0\", model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
    "   \n",
    "    # If model not saved locally, download and save it first\n",
    "    if not os.path.exists(model_path):\n",
    "        save_gemma_locally(save_path=model_path, model_name=model_name)\n",
    "\n",
    "    print(\"ðŸ§  Loading TinyLlama model with disk offloading...\")\n",
    "\n",
    "    # Load tokenizer from local folder\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, token=token, trust_remote_code=True)\n",
    "\n",
    "    # Load model using disk offloading\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        offload_folder=\"./offload\",  # Folder for offloaded weights\n",
    "        offload_state_dict=True      # Enable full offloading\n",
    "    )\n",
    "\n",
    "    # Alternatively: use `accelerate` for more control\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model,\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"./offload\",\n",
    "        dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Create text generation pipeline\n",
    "    tinyllama_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=tinyllama_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ff570e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Loading TinyLlama model with disk offloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "llm = load_tinyLlama_pipeline(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "qns1 = db_chain(\"How many t-shirts do we have left for nike in extra small size and white color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qns2 = db_chain.run(\"SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c493ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = db_chain.invoke(\"SELECT SUM(price*stock_quantity) FROM t_shirts WHERE size = 'S'\")\n",
    "qns2 = response[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
    "qns3 = db_chain(\"How many records are in the database?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
